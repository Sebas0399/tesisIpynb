{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ImageDataGenerator' from 'keras.preprocessing.image' (c:\\Users\\Sebas\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\keras\\preprocessing\\image\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mLoadData\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoadData\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ImageDataGenerator\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ImageDataGenerator' from 'keras.preprocessing.image' (c:\\Users\\Sebas\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\keras\\preprocessing\\image\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from configparser import ConfigParser\n",
    "from keras.metrics import Precision,Recall,AUC,KLDivergence,Poisson,MeanAbsoluteError,F1Score,FBetaScore,CategoricalAccuracy\n",
    "from keras.callbacks import EarlyStopping\n",
    "import scipy\n",
    "from ModelSelector import  ModelSelector\n",
    "import keras_cv\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner as kt\n",
    "from keras.callbacks import Callback\n",
    "from keras_cv.models import ImageClassifier\n",
    "from keras_cv.layers import RandAugment\n",
    "from keras.models import Sequential\n",
    "from LoadData import LoadData\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos las configuraciones necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "configfile_name = \"config.ini\" \n",
    "config= ConfigParser()\n",
    "config.read(configfile_name)\n",
    "image_size = int(config.get(\"dataset\",\"image_size\"))\n",
    "color_mode=config.get(\"dataset\",\"color_mode\")\n",
    "batch_size = int(config.get(\"dataset\",\"batch_size\"))\n",
    "base_dir=config.get(\"dataset\",\"base_dir\")\n",
    "dest_dir=config.get(\"dataset\",\"dest_dir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "0.2\n",
      "0.2\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[WinError 145] The directory is not empty: 'Nuevo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mSplitData\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SplitData\n\u001b[0;32m      2\u001b[0m data\u001b[38;5;241m=\u001b[39mSplitData()\n\u001b[1;32m----> 3\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sebas\\Desktop\\tesisIpynb\\SplitData.py:51\u001b[0m, in \u001b[0;36mSplitData.split\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m             shutil\u001b[38;5;241m.\u001b[39mmove(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(class_dir, img), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, class_name, img))\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNuevo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 145] The directory is not empty: 'Nuevo'"
     ]
    }
   ],
   "source": [
    "from SplitData import SplitData\n",
    "data=SplitData()\n",
    "data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LoadData=LoadData(image_size,color_mode,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1161 files belonging to 3 classes.\n",
      "Found 60 files belonging to 3 classes.\n",
      "Found 231 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds=LoadData.load_train_data(dest_dir)\n",
    "test_ds=LoadData.load_test_data(dest_dir)\n",
    "val_ds=LoadData.load_validation_data(dest_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrar_imagenes(dataset):\n",
    "    \"\"\"\n",
    "    Muestra las primeras 9 imágenes de un lote del dataset junto con sus etiquetas.\n",
    "    Las etiquetas se infieren automáticamente desde el atributo `class_names` del dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (tf.data.Dataset): Dataset cargado con image_dataset_from_directory.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Obtener nombres de las clases directamente del dataset\n",
    "        class_names = dataset.class_names\n",
    "    except AttributeError:\n",
    "        raise ValueError(\"El dataset proporcionado no contiene información de `class_names`.\")\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for images, labels in dataset.take(1):  # Tomar un lote del dataset\n",
    "        for i in range(9):  # Mostrar las primeras 9 imágenes del lote\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "            # Convertir etiquetas categóricas en nombres de clases\n",
    "            plt.title(class_names[labels[i].numpy().argmax()])\n",
    "            plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos las diferentes clases del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mostrar_imagenes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmostrar_imagenes\u001b[49m(train_ds)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mostrar_imagenes' is not defined"
     ]
    }
   ],
   "source": [
    "mostrar_imagenes(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionamos algun modelo preentrenado (terminacion_pretrained) o su arquitectura de capas de nuestra clase Model Selector.\n",
    "- resnet50\n",
    "- resnet50_pretained\n",
    "- resnet152\n",
    "- cspdarknet\n",
    "- cspdarknet_pretained\n",
    "- densenet169\n",
    "- densenet169_pretrained\n",
    "- efficientnet\n",
    "- efficientnet_V1\n",
    "- efficientnet_V2_pretained\n",
    "- efficientnet_V2\n",
    "- mobilenet_V3_pretrained\n",
    "- mobilenet_V3\n",
    "- yolo_v8\n",
    "- yolo_v8_pretained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/keras/efficientnetv2/keras/efficientnetv2_s_imagenet/2/download/config.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.79k/1.79k [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Sebas\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:187: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/keras/efficientnetv2/keras/efficientnetv2_s_imagenet/2/download/model.weights.h5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78.6M/78.6M [00:08<00:00, 9.62MB/s]\n"
     ]
    }
   ],
   "source": [
    "backbone = ModelSelector(image_size=image_size, color_space=color_mode, model='efficientnet_V2_pretained').load_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras_cv.models.ImageClassifier(\n",
    "    backbone=backbone,\n",
    "    num_classes=3,\n",
    "    activation=\"softmax\",\n",
    "    pooling=\"avg\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [01h 00m 34s]\n",
      "val_loss: 0.1350744217634201\n",
      "\n",
      "Best val_loss So Far: 0.08574290573596954\n",
      "Total elapsed time: 05h 56m 29s\n",
      "Mejores hiperparámetros: {'augmentations_per_image': 4, 'magnitude': 0.5, 'magnitude_stddev': 0.25, 'rate': 0.6}\n"
     ]
    }
   ],
   "source": [
    "def build_model(hp, X_train):\n",
    "    # Crear el objeto RandAugment con hiperparámetros ajustables\n",
    "    augmenter = RandAugment(\n",
    "        value_range=(0, 255),\n",
    "        augmentations_per_image=hp.Int('augmentations_per_image', min_value=2, max_value=5, step=1, default=3),\n",
    "        magnitude=hp.Float('magnitude', min_value=0.1, max_value=1.0, step=0.1, default=0.5),\n",
    "        magnitude_stddev=hp.Float('magnitude_stddev', min_value=0.05, max_value=0.3, step=0.05, default=0.15),\n",
    "        rate=hp.Float('rate', min_value=0.5, max_value=1.0, step=0.1, default=0.9),\n",
    "        geometric=True\n",
    "    )\n",
    "\n",
    "    # Crear el modelo base\n",
    "    model = ImageClassifier(\n",
    "        backbone=backbone,\n",
    "        num_classes=3,\n",
    "        activation=\"softmax\",\n",
    "        pooling=\"avg\"\n",
    "    )\n",
    "\n",
    "    # Aplicar las transformaciones de RandAugment a los datos de entrenamiento y validación\n",
    "    X_train_aug = X_train.map(lambda x, y: (augmenter(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # Compilar el modelo final\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=keras.losses.CategoricalCrossentropy(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model, X_train_aug\n",
    "\n",
    "# Crear el tuner de Keras\n",
    "tuner = kt.RandomSearch(\n",
    "    lambda hp: build_model(hp, X_train=train_ds)[0],  # Devuelve solo el modelo\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    overwrite=True,\n",
    "    directory='rand_augment_tuning_effi'\n",
    ")\n",
    "\n",
    "# Ejecutar la búsqueda de hiperparámetros\n",
    "tuner.search(x=train_ds, validation_data=val_ds, epochs=10)\n",
    "\n",
    "# Obtener los mejores hiperparámetros\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f'Mejores hiperparámetros: {best_hps.values}')\n",
    "\n",
    "# Crear el modelo final con los mejores hiperparámetros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model, X_train_aug = build_model(best_hps, X_train=train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir=\"./ckpt_effi_rand_augment\"\n",
    "checkpoint_dir=log_dir+'/ckpt'\n",
    "checkpoint_filepath = checkpoint_dir+'/checkpoint_{epoch}.keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,verbose=1,    save_best_only=True,\n",
    ")\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
    "csv_logger = tf.keras.callbacks.CSVLogger('training_log_effi_random.csv', append=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_files = os.listdir(checkpoint_dir)\n",
    "checkpoint_files = [file for file in checkpoint_files if file.startswith('checkpoint_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(checkpoint_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    metrics=[  Precision(name='precision'),\n",
    "        Recall(name='recall'),\n",
    "        AUC(name='auc'),\n",
    "        MeanAbsoluteError(name='mae'),\n",
    "        CategoricalAccuracy(name=\"categorical_accuracy\", dtype=None),\n",
    "         \n",
    "         Poisson(name=\"poisson\", dtype=None),\n",
    "         ]\n",
    "       \n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hay checkpoints.\n",
      "Epoch 1/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - auc: 0.5280 - categorical_accuracy: 0.4058 - loss: 1.3696 - mae: 0.4445 - poisson: 0.7899 - precision: 0.3898 - recall: 0.1402\n",
      "Epoch 1: val_loss improved from inf to 1.67336, saving model to ./ckpt_effi_rand_augment/ckpt/checkpoint_1.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m503s\u001b[0m 10s/step - auc: 0.5286 - categorical_accuracy: 0.4064 - loss: 1.3689 - mae: 0.4442 - poisson: 0.7896 - precision: 0.3918 - recall: 0.1414 - val_auc: 0.6281 - val_categorical_accuracy: 0.5411 - val_loss: 1.6734 - val_mae: 0.3891 - val_poisson: 0.8893 - val_precision: 0.5805 - val_recall: 0.4372 - learning_rate: 1.0000e-05\n",
      "Epoch 2/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - auc: 0.6190 - categorical_accuracy: 0.4706 - loss: 1.1411 - mae: 0.4162 - poisson: 0.7137 - precision: 0.5744 - recall: 0.2197\n",
      "Epoch 2: val_loss improved from 1.67336 to 0.99705, saving model to ./ckpt_effi_rand_augment/ckpt/checkpoint_2.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m353s\u001b[0m 9s/step - auc: 0.6198 - categorical_accuracy: 0.4714 - loss: 1.1397 - mae: 0.4159 - poisson: 0.7132 - precision: 0.5756 - recall: 0.2203 - val_auc: 0.7886 - val_categorical_accuracy: 0.6883 - val_loss: 0.9971 - val_mae: 0.3206 - val_poisson: 0.6657 - val_precision: 0.7647 - val_recall: 0.5628 - learning_rate: 1.0000e-05\n",
      "Epoch 3/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - auc: 0.6945 - categorical_accuracy: 0.5269 - loss: 1.0282 - mae: 0.3874 - poisson: 0.6761 - precision: 0.6943 - recall: 0.3172\n",
      "Epoch 3: val_loss improved from 0.99705 to 0.72287, saving model to ./ckpt_effi_rand_augment/ckpt/checkpoint_3.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 9s/step - auc: 0.6949 - categorical_accuracy: 0.5273 - loss: 1.0274 - mae: 0.3872 - poisson: 0.6758 - precision: 0.6944 - recall: 0.3177 - val_auc: 0.8723 - val_categorical_accuracy: 0.7489 - val_loss: 0.7229 - val_mae: 0.2605 - val_poisson: 0.5743 - val_precision: 0.8424 - val_recall: 0.6710 - learning_rate: 1.0000e-05\n",
      "Epoch 4/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - auc: 0.7635 - categorical_accuracy: 0.5813 - loss: 0.9061 - mae: 0.3601 - poisson: 0.6354 - precision: 0.7598 - recall: 0.3803\n",
      "Epoch 4: val_loss improved from 0.72287 to 0.48068, saving model to ./ckpt_effi_rand_augment/ckpt/checkpoint_4.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 9s/step - auc: 0.7638 - categorical_accuracy: 0.5815 - loss: 0.9056 - mae: 0.3600 - poisson: 0.6352 - precision: 0.7601 - recall: 0.3805 - val_auc: 0.9632 - val_categorical_accuracy: 0.8268 - val_loss: 0.4807 - val_mae: 0.2207 - val_poisson: 0.4936 - val_precision: 0.9061 - val_recall: 0.7100 - learning_rate: 1.0000e-05\n",
      "Epoch 5/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - auc: 0.8349 - categorical_accuracy: 0.6663 - loss: 0.8021 - mae: 0.3314 - poisson: 0.6007 - precision: 0.8127 - recall: 0.4503\n",
      "Epoch 5: val_loss improved from 0.48068 to 0.35526, saving model to ./ckpt_effi_rand_augment/ckpt/checkpoint_5.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m355s\u001b[0m 9s/step - auc: 0.8348 - categorical_accuracy: 0.6661 - loss: 0.8023 - mae: 0.3315 - poisson: 0.6008 - precision: 0.8128 - recall: 0.4503 - val_auc: 0.9876 - val_categorical_accuracy: 0.9221 - val_loss: 0.3553 - val_mae: 0.1733 - val_poisson: 0.4518 - val_precision: 0.9752 - val_recall: 0.8528 - learning_rate: 1.0000e-05\n",
      "Epoch 6/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - auc: 0.8416 - categorical_accuracy: 0.6697 - loss: 0.7822 - mae: 0.3226 - poisson: 0.5941 - precision: 0.8194 - recall: 0.4734\n",
      "Epoch 6: val_loss improved from 0.35526 to 0.29708, saving model to ./ckpt_effi_rand_augment/ckpt/checkpoint_6.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m360s\u001b[0m 10s/step - auc: 0.8418 - categorical_accuracy: 0.6699 - loss: 0.7820 - mae: 0.3226 - poisson: 0.5940 - precision: 0.8191 - recall: 0.4733 - val_auc: 0.9910 - val_categorical_accuracy: 0.9437 - val_loss: 0.2971 - val_mae: 0.1457 - val_poisson: 0.4324 - val_precision: 0.9709 - val_recall: 0.8658 - learning_rate: 1.0000e-05\n",
      "Epoch 7/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - auc: 0.8719 - categorical_accuracy: 0.7140 - loss: 0.7274 - mae: 0.3049 - poisson: 0.5758 - precision: 0.8384 - recall: 0.5224\n",
      "Epoch 7: val_loss improved from 0.29708 to 0.27718, saving model to ./ckpt_effi_rand_augment/ckpt/checkpoint_7.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m365s\u001b[0m 10s/step - auc: 0.8719 - categorical_accuracy: 0.7139 - loss: 0.7276 - mae: 0.3049 - poisson: 0.5759 - precision: 0.8385 - recall: 0.5224 - val_auc: 0.9901 - val_categorical_accuracy: 0.9394 - val_loss: 0.2772 - val_mae: 0.1336 - val_poisson: 0.4257 - val_precision: 0.9761 - val_recall: 0.8831 - learning_rate: 1.0000e-05\n",
      "Epoch 8/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - auc: 0.8714 - categorical_accuracy: 0.7106 - loss: 0.7233 - mae: 0.3001 - poisson: 0.5744 - precision: 0.8414 - recall: 0.5273\n",
      "Epoch 8: val_loss improved from 0.27718 to 0.23304, saving model to ./ckpt_effi_rand_augment/ckpt/checkpoint_8.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m373s\u001b[0m 10s/step - auc: 0.8714 - categorical_accuracy: 0.7106 - loss: 0.7231 - mae: 0.3000 - poisson: 0.5744 - precision: 0.8411 - recall: 0.5273 - val_auc: 0.9924 - val_categorical_accuracy: 0.9437 - val_loss: 0.2330 - val_mae: 0.1113 - val_poisson: 0.4110 - val_precision: 0.9720 - val_recall: 0.9004 - learning_rate: 1.0000e-05\n",
      "Epoch 9/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - auc: 0.8927 - categorical_accuracy: 0.7337 - loss: 0.6672 - mae: 0.2801 - poisson: 0.5557 - precision: 0.8652 - recall: 0.5539\n",
      "Epoch 9: val_loss improved from 0.23304 to 0.22600, saving model to ./ckpt_effi_rand_augment/ckpt/checkpoint_9.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m365s\u001b[0m 10s/step - auc: 0.8927 - categorical_accuracy: 0.7339 - loss: 0.6672 - mae: 0.2801 - poisson: 0.5557 - precision: 0.8652 - recall: 0.5539 - val_auc: 0.9917 - val_categorical_accuracy: 0.9437 - val_loss: 0.2260 - val_mae: 0.1058 - val_poisson: 0.4087 - val_precision: 0.9676 - val_recall: 0.9048 - learning_rate: 1.0000e-05\n",
      "Epoch 10/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - auc: 0.8822 - categorical_accuracy: 0.7169 - loss: 0.6791 - mae: 0.2764 - poisson: 0.5597 - precision: 0.8283 - recall: 0.5979\n",
      "Epoch 10: val_loss improved from 0.22600 to 0.21487, saving model to ./ckpt_effi_rand_augment/ckpt/checkpoint_10.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m363s\u001b[0m 10s/step - auc: 0.8825 - categorical_accuracy: 0.7173 - loss: 0.6788 - mae: 0.2764 - poisson: 0.5596 - precision: 0.8288 - recall: 0.5977 - val_auc: 0.9915 - val_categorical_accuracy: 0.9394 - val_loss: 0.2149 - val_mae: 0.0985 - val_poisson: 0.4050 - val_precision: 0.9630 - val_recall: 0.9004 - learning_rate: 1.0000e-05\n",
      "Epoch 11/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - auc: 0.8905 - categorical_accuracy: 0.7261 - loss: 0.6641 - mae: 0.2735 - poisson: 0.5547 - precision: 0.8495 - recall: 0.5935\n",
      "Epoch 11: val_loss improved from 0.21487 to 0.20150, saving model to ./ckpt_effi_rand_augment/ckpt/checkpoint_11.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m355s\u001b[0m 9s/step - auc: 0.8904 - categorical_accuracy: 0.7262 - loss: 0.6642 - mae: 0.2734 - poisson: 0.5547 - precision: 0.8493 - recall: 0.5936 - val_auc: 0.9918 - val_categorical_accuracy: 0.9437 - val_loss: 0.2015 - val_mae: 0.0903 - val_poisson: 0.4005 - val_precision: 0.9633 - val_recall: 0.9091 - learning_rate: 1.0000e-05\n",
      "Epoch 12/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - auc: 0.8984 - categorical_accuracy: 0.7437 - loss: 0.6431 - mae: 0.2671 - poisson: 0.5477 - precision: 0.8575 - recall: 0.6016\n",
      "Epoch 12: val_loss improved from 0.20150 to 0.20113, saving model to ./ckpt_effi_rand_augment/ckpt/checkpoint_12.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m369s\u001b[0m 10s/step - auc: 0.8983 - categorical_accuracy: 0.7439 - loss: 0.6431 - mae: 0.2670 - poisson: 0.5477 - precision: 0.8573 - recall: 0.6016 - val_auc: 0.9917 - val_categorical_accuracy: 0.9394 - val_loss: 0.2011 - val_mae: 0.0902 - val_poisson: 0.4004 - val_precision: 0.9630 - val_recall: 0.9004 - learning_rate: 1.0000e-05\n",
      "Epoch 13/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - auc: 0.8727 - categorical_accuracy: 0.7100 - loss: 0.6954 - mae: 0.2791 - poisson: 0.5651 - precision: 0.8014 - recall: 0.5410\n",
      "Epoch 13: val_loss improved from 0.20113 to 0.19121, saving model to ./ckpt_effi_rand_augment/ckpt/checkpoint_13.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 10s/step - auc: 0.8729 - categorical_accuracy: 0.7103 - loss: 0.6950 - mae: 0.2789 - poisson: 0.5650 - precision: 0.8017 - recall: 0.5418 - val_auc: 0.9912 - val_categorical_accuracy: 0.9437 - val_loss: 0.1912 - val_mae: 0.0809 - val_poisson: 0.3971 - val_precision: 0.9635 - val_recall: 0.9134 - learning_rate: 1.0000e-05\n",
      "Epoch 14/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - auc: 0.9075 - categorical_accuracy: 0.7554 - loss: 0.6093 - mae: 0.2503 - poisson: 0.5364 - precision: 0.8475 - recall: 0.6225\n",
      "Epoch 14: val_loss improved from 0.19121 to 0.18261, saving model to ./ckpt_effi_rand_augment/ckpt/checkpoint_14.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m366s\u001b[0m 10s/step - auc: 0.9073 - categorical_accuracy: 0.7550 - loss: 0.6100 - mae: 0.2505 - poisson: 0.5367 - precision: 0.8474 - recall: 0.6221 - val_auc: 0.9904 - val_categorical_accuracy: 0.9394 - val_loss: 0.1826 - val_mae: 0.0743 - val_poisson: 0.3942 - val_precision: 0.9636 - val_recall: 0.9177 - learning_rate: 1.0000e-05\n",
      "Epoch 15/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - auc: 0.9017 - categorical_accuracy: 0.7337 - loss: 0.6231 - mae: 0.2583 - poisson: 0.5410 - precision: 0.8340 - recall: 0.6066\n",
      "Epoch 15: val_loss improved from 0.18261 to 0.18055, saving model to ./ckpt_effi_rand_augment/ckpt/checkpoint_15.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 10s/step - auc: 0.9018 - categorical_accuracy: 0.7339 - loss: 0.6231 - mae: 0.2582 - poisson: 0.5410 - precision: 0.8342 - recall: 0.6070 - val_auc: 0.9908 - val_categorical_accuracy: 0.9437 - val_loss: 0.1806 - val_mae: 0.0743 - val_poisson: 0.3935 - val_precision: 0.9636 - val_recall: 0.9177 - learning_rate: 1.0000e-05\n",
      "Epoch 16/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - auc: 0.9117 - categorical_accuracy: 0.7747 - loss: 0.5983 - mae: 0.2476 - poisson: 0.5328 - precision: 0.8516 - recall: 0.6389\n",
      "Epoch 16: val_loss improved from 0.18055 to 0.17570, saving model to ./ckpt_effi_rand_augment/ckpt/checkpoint_16.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m368s\u001b[0m 10s/step - auc: 0.9115 - categorical_accuracy: 0.7745 - loss: 0.5985 - mae: 0.2476 - poisson: 0.5328 - precision: 0.8515 - recall: 0.6391 - val_auc: 0.9904 - val_categorical_accuracy: 0.9437 - val_loss: 0.1757 - val_mae: 0.0699 - val_poisson: 0.3919 - val_precision: 0.9591 - val_recall: 0.9134 - learning_rate: 1.0000e-05\n",
      "Epoch 17/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - auc: 0.9123 - categorical_accuracy: 0.7419 - loss: 0.5915 - mae: 0.2452 - poisson: 0.5305 - precision: 0.8621 - recall: 0.6309\n",
      "Epoch 17: val_loss did not improve from 0.17570\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 9s/step - auc: 0.9120 - categorical_accuracy: 0.7419 - loss: 0.5922 - mae: 0.2453 - poisson: 0.5307 - precision: 0.8615 - recall: 0.6308 - val_auc: 0.9918 - val_categorical_accuracy: 0.9307 - val_loss: 0.1820 - val_mae: 0.0746 - val_poisson: 0.3940 - val_precision: 0.9548 - val_recall: 0.9134 - learning_rate: 1.0000e-05\n",
      "Epoch 18/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - auc: 0.9079 - categorical_accuracy: 0.7455 - loss: 0.6035 - mae: 0.2466 - poisson: 0.5345 - precision: 0.8493 - recall: 0.6353\n",
      "Epoch 18: val_loss did not improve from 0.17570\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m354s\u001b[0m 9s/step - auc: 0.9080 - categorical_accuracy: 0.7456 - loss: 0.6036 - mae: 0.2466 - poisson: 0.5345 - precision: 0.8493 - recall: 0.6354 - val_auc: 0.9914 - val_categorical_accuracy: 0.9394 - val_loss: 0.1847 - val_mae: 0.0765 - val_poisson: 0.3949 - val_precision: 0.9633 - val_recall: 0.9091 - learning_rate: 1.0000e-05\n",
      "Epoch 19/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - auc: 0.9113 - categorical_accuracy: 0.7671 - loss: 0.5975 - mae: 0.2389 - poisson: 0.5325 - precision: 0.8609 - recall: 0.6690\n",
      "Epoch 19: val_loss did not improve from 0.17570\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m350s\u001b[0m 9s/step - auc: 0.9113 - categorical_accuracy: 0.7671 - loss: 0.5974 - mae: 0.2388 - poisson: 0.5325 - precision: 0.8607 - recall: 0.6689 - val_auc: 0.9900 - val_categorical_accuracy: 0.9307 - val_loss: 0.1837 - val_mae: 0.0719 - val_poisson: 0.3945 - val_precision: 0.9505 - val_recall: 0.9134 - learning_rate: 1.0000e-05\n",
      "Epoch 20/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - auc: 0.9221 - categorical_accuracy: 0.7794 - loss: 0.5621 - mae: 0.2330 - poisson: 0.5207 - precision: 0.8697 - recall: 0.6644\n",
      "Epoch 20: val_loss improved from 0.17570 to 0.17479, saving model to ./ckpt_effi_rand_augment/ckpt/checkpoint_20.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m357s\u001b[0m 9s/step - auc: 0.9218 - categorical_accuracy: 0.7789 - loss: 0.5629 - mae: 0.2332 - poisson: 0.5210 - precision: 0.8692 - recall: 0.6641 - val_auc: 0.9906 - val_categorical_accuracy: 0.9351 - val_loss: 0.1748 - val_mae: 0.0677 - val_poisson: 0.3916 - val_precision: 0.9636 - val_recall: 0.9177 - learning_rate: 1.0000e-05\n",
      "Epoch 21/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - auc: 0.9239 - categorical_accuracy: 0.7744 - loss: 0.5608 - mae: 0.2328 - poisson: 0.5203 - precision: 0.8812 - recall: 0.6703\n",
      "Epoch 21: val_loss did not improve from 0.17479\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m355s\u001b[0m 9s/step - auc: 0.9236 - categorical_accuracy: 0.7740 - loss: 0.5615 - mae: 0.2330 - poisson: 0.5205 - precision: 0.8804 - recall: 0.6700 - val_auc: 0.9904 - val_categorical_accuracy: 0.9481 - val_loss: 0.1812 - val_mae: 0.0720 - val_poisson: 0.3937 - val_precision: 0.9635 - val_recall: 0.9134 - learning_rate: 1.0000e-05\n",
      "Epoch 22/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - auc: 0.9044 - categorical_accuracy: 0.7645 - loss: 0.6068 - mae: 0.2403 - poisson: 0.5356 - precision: 0.8344 - recall: 0.6371\n",
      "Epoch 22: val_loss improved from 0.17479 to 0.16816, saving model to ./ckpt_effi_rand_augment/ckpt/checkpoint_22.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 10s/step - auc: 0.9048 - categorical_accuracy: 0.7651 - loss: 0.6058 - mae: 0.2401 - poisson: 0.5353 - precision: 0.8350 - recall: 0.6375 - val_auc: 0.9908 - val_categorical_accuracy: 0.9351 - val_loss: 0.1682 - val_mae: 0.0641 - val_poisson: 0.3894 - val_precision: 0.9591 - val_recall: 0.9134 - learning_rate: 1.0000e-05\n",
      "Epoch 23/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - auc: 0.9136 - categorical_accuracy: 0.7591 - loss: 0.5777 - mae: 0.2347 - poisson: 0.5259 - precision: 0.8388 - recall: 0.6533\n",
      "Epoch 23: val_loss did not improve from 0.16816\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m361s\u001b[0m 10s/step - auc: 0.9136 - categorical_accuracy: 0.7592 - loss: 0.5778 - mae: 0.2347 - poisson: 0.5259 - precision: 0.8388 - recall: 0.6534 - val_auc: 0.9903 - val_categorical_accuracy: 0.9394 - val_loss: 0.1710 - val_mae: 0.0640 - val_poisson: 0.3903 - val_precision: 0.9636 - val_recall: 0.9177 - learning_rate: 1.0000e-05\n",
      "Epoch 24/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - auc: 0.9312 - categorical_accuracy: 0.7879 - loss: 0.5242 - mae: 0.2164 - poisson: 0.5081 - precision: 0.8636 - recall: 0.6969\n",
      "Epoch 24: val_loss did not improve from 0.16816\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m358s\u001b[0m 10s/step - auc: 0.9313 - categorical_accuracy: 0.7881 - loss: 0.5241 - mae: 0.2164 - poisson: 0.5080 - precision: 0.8639 - recall: 0.6970 - val_auc: 0.9898 - val_categorical_accuracy: 0.9351 - val_loss: 0.1775 - val_mae: 0.0661 - val_poisson: 0.3925 - val_precision: 0.9550 - val_recall: 0.9177 - learning_rate: 1.0000e-05\n",
      "Epoch 25/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - auc: 0.9174 - categorical_accuracy: 0.7490 - loss: 0.5573 - mae: 0.2246 - poisson: 0.5191 - precision: 0.8439 - recall: 0.6710\n",
      "Epoch 25: val_loss did not improve from 0.16816\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m351s\u001b[0m 9s/step - auc: 0.9174 - categorical_accuracy: 0.7493 - loss: 0.5574 - mae: 0.2245 - poisson: 0.5191 - precision: 0.8439 - recall: 0.6714 - val_auc: 0.9901 - val_categorical_accuracy: 0.9394 - val_loss: 0.1736 - val_mae: 0.0631 - val_poisson: 0.3912 - val_precision: 0.9638 - val_recall: 0.9221 - learning_rate: 1.0000e-05\n",
      "Epoch 26/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - auc: 0.9381 - categorical_accuracy: 0.8145 - loss: 0.5034 - mae: 0.2041 - poisson: 0.5011 - precision: 0.8897 - recall: 0.7250\n",
      "Epoch 26: val_loss did not improve from 0.16816\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 9s/step - auc: 0.9380 - categorical_accuracy: 0.8143 - loss: 0.5039 - mae: 0.2043 - poisson: 0.5013 - precision: 0.8895 - recall: 0.7246 - val_auc: 0.9907 - val_categorical_accuracy: 0.9394 - val_loss: 0.1691 - val_mae: 0.0621 - val_poisson: 0.3897 - val_precision: 0.9548 - val_recall: 0.9134 - learning_rate: 1.0000e-05\n",
      "Epoch 27/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - auc: 0.9165 - categorical_accuracy: 0.7601 - loss: 0.5664 - mae: 0.2245 - poisson: 0.5221 - precision: 0.8512 - recall: 0.6657\n",
      "Epoch 27: val_loss improved from 0.16816 to 0.15866, saving model to ./ckpt_effi_rand_augment/ckpt/checkpoint_27.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m360s\u001b[0m 10s/step - auc: 0.9164 - categorical_accuracy: 0.7599 - loss: 0.5668 - mae: 0.2245 - poisson: 0.5223 - precision: 0.8508 - recall: 0.6657 - val_auc: 0.9916 - val_categorical_accuracy: 0.9437 - val_loss: 0.1587 - val_mae: 0.0567 - val_poisson: 0.3862 - val_precision: 0.9595 - val_recall: 0.9221 - learning_rate: 1.0000e-05\n",
      "Epoch 28/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - auc: 0.9208 - categorical_accuracy: 0.7718 - loss: 0.5590 - mae: 0.2248 - poisson: 0.5197 - precision: 0.8477 - recall: 0.6735\n",
      "Epoch 28: val_loss did not improve from 0.15866\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m350s\u001b[0m 9s/step - auc: 0.9208 - categorical_accuracy: 0.7718 - loss: 0.5589 - mae: 0.2247 - poisson: 0.5196 - precision: 0.8475 - recall: 0.6737 - val_auc: 0.9905 - val_categorical_accuracy: 0.9437 - val_loss: 0.1694 - val_mae: 0.0599 - val_poisson: 0.3898 - val_precision: 0.9595 - val_recall: 0.9221 - learning_rate: 1.0000e-05\n",
      "Epoch 29/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - auc: 0.9193 - categorical_accuracy: 0.7696 - loss: 0.5541 - mae: 0.2175 - poisson: 0.5180 - precision: 0.8422 - recall: 0.6921\n",
      "Epoch 29: val_loss did not improve from 0.15866\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 9s/step - auc: 0.9192 - categorical_accuracy: 0.7695 - loss: 0.5543 - mae: 0.2175 - poisson: 0.5181 - precision: 0.8419 - recall: 0.6919 - val_auc: 0.9907 - val_categorical_accuracy: 0.9351 - val_loss: 0.1705 - val_mae: 0.0619 - val_poisson: 0.3902 - val_precision: 0.9636 - val_recall: 0.9177 - learning_rate: 1.0000e-05\n",
      "Epoch 30/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - auc: 0.9249 - categorical_accuracy: 0.7756 - loss: 0.5412 - mae: 0.2097 - poisson: 0.5137 - precision: 0.8571 - recall: 0.6929\n",
      "Epoch 30: val_loss did not improve from 0.15866\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m355s\u001b[0m 9s/step - auc: 0.9250 - categorical_accuracy: 0.7756 - loss: 0.5409 - mae: 0.2096 - poisson: 0.5136 - precision: 0.8571 - recall: 0.6930 - val_auc: 0.9899 - val_categorical_accuracy: 0.9394 - val_loss: 0.1766 - val_mae: 0.0647 - val_poisson: 0.3922 - val_precision: 0.9550 - val_recall: 0.9177 - learning_rate: 1.0000e-05\n",
      "Epoch 31/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - auc: 0.9201 - categorical_accuracy: 0.7675 - loss: 0.5481 - mae: 0.2193 - poisson: 0.5160 - precision: 0.8400 - recall: 0.6777\n",
      "Epoch 31: val_loss did not improve from 0.15866\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m368s\u001b[0m 10s/step - auc: 0.9201 - categorical_accuracy: 0.7675 - loss: 0.5483 - mae: 0.2193 - poisson: 0.5161 - precision: 0.8400 - recall: 0.6778 - val_auc: 0.9902 - val_categorical_accuracy: 0.9394 - val_loss: 0.1706 - val_mae: 0.0619 - val_poisson: 0.3902 - val_precision: 0.9550 - val_recall: 0.9177 - learning_rate: 1.0000e-05\n",
      "Epoch 32/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - auc: 0.9282 - categorical_accuracy: 0.7789 - loss: 0.5259 - mae: 0.2119 - poisson: 0.5086 - precision: 0.8467 - recall: 0.6977\n",
      "Epoch 32: val_loss did not improve from 0.15866\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m370s\u001b[0m 10s/step - auc: 0.9282 - categorical_accuracy: 0.7789 - loss: 0.5261 - mae: 0.2120 - poisson: 0.5087 - precision: 0.8470 - recall: 0.6975 - val_auc: 0.9902 - val_categorical_accuracy: 0.9394 - val_loss: 0.1733 - val_mae: 0.0591 - val_poisson: 0.3911 - val_precision: 0.9550 - val_recall: 0.9177 - learning_rate: 1.0000e-05\n",
      "Epoch 33/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - auc: 0.9325 - categorical_accuracy: 0.7882 - loss: 0.5156 - mae: 0.2100 - poisson: 0.5052 - precision: 0.8774 - recall: 0.6927\n",
      "Epoch 33: val_loss did not improve from 0.15866\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m369s\u001b[0m 10s/step - auc: 0.9324 - categorical_accuracy: 0.7880 - loss: 0.5157 - mae: 0.2101 - poisson: 0.5052 - precision: 0.8769 - recall: 0.6928 - val_auc: 0.9905 - val_categorical_accuracy: 0.9394 - val_loss: 0.1728 - val_mae: 0.0592 - val_poisson: 0.3909 - val_precision: 0.9550 - val_recall: 0.9177 - learning_rate: 2.0000e-06\n",
      "Epoch 34/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - auc: 0.9224 - categorical_accuracy: 0.7677 - loss: 0.5442 - mae: 0.2173 - poisson: 0.5147 - precision: 0.8537 - recall: 0.6803\n",
      "Epoch 34: val_loss did not improve from 0.15866\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m368s\u001b[0m 10s/step - auc: 0.9224 - categorical_accuracy: 0.7678 - loss: 0.5441 - mae: 0.2172 - poisson: 0.5147 - precision: 0.8535 - recall: 0.6803 - val_auc: 0.9906 - val_categorical_accuracy: 0.9437 - val_loss: 0.1690 - val_mae: 0.0576 - val_poisson: 0.3897 - val_precision: 0.9507 - val_recall: 0.9177 - learning_rate: 2.0000e-06\n",
      "Epoch 35/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - auc: 0.9267 - categorical_accuracy: 0.7715 - loss: 0.5324 - mae: 0.2141 - poisson: 0.5108 - precision: 0.8421 - recall: 0.6837\n",
      "Epoch 35: val_loss did not improve from 0.15866\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 10s/step - auc: 0.9267 - categorical_accuracy: 0.7717 - loss: 0.5325 - mae: 0.2140 - poisson: 0.5108 - precision: 0.8423 - recall: 0.6839 - val_auc: 0.9903 - val_categorical_accuracy: 0.9351 - val_loss: 0.1697 - val_mae: 0.0588 - val_poisson: 0.3899 - val_precision: 0.9593 - val_recall: 0.9177 - learning_rate: 2.0000e-06\n",
      "Epoch 36/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - auc: 0.9336 - categorical_accuracy: 0.7964 - loss: 0.5147 - mae: 0.2087 - poisson: 0.5049 - precision: 0.8627 - recall: 0.7210\n",
      "Epoch 36: val_loss did not improve from 0.15866\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m353s\u001b[0m 9s/step - auc: 0.9335 - categorical_accuracy: 0.7962 - loss: 0.5148 - mae: 0.2087 - poisson: 0.5049 - precision: 0.8626 - recall: 0.7208 - val_auc: 0.9899 - val_categorical_accuracy: 0.9351 - val_loss: 0.1757 - val_mae: 0.0579 - val_poisson: 0.3919 - val_precision: 0.9550 - val_recall: 0.9177 - learning_rate: 2.0000e-06\n",
      "Epoch 37/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - auc: 0.9459 - categorical_accuracy: 0.8223 - loss: 0.4741 - mae: 0.1977 - poisson: 0.4914 - precision: 0.8816 - recall: 0.7238\n",
      "Epoch 37: val_loss did not improve from 0.15866\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m351s\u001b[0m 9s/step - auc: 0.9456 - categorical_accuracy: 0.8214 - loss: 0.4752 - mae: 0.1979 - poisson: 0.4917 - precision: 0.8810 - recall: 0.7232 - val_auc: 0.9907 - val_categorical_accuracy: 0.9437 - val_loss: 0.1679 - val_mae: 0.0596 - val_poisson: 0.3893 - val_precision: 0.9596 - val_recall: 0.9264 - learning_rate: 2.0000e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2871efdf1f0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if checkpoint_files:\n",
    "    checkpoint_files.sort(key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
    "    latest_checkpoint = checkpoint_files[-1]\n",
    "    last_epoch = int(latest_checkpoint.split('_')[1].split('.')[0])\n",
    "    final_model = tf.keras.models.load_model(os.path.join(checkpoint_dir, latest_checkpoint))\n",
    "else:\n",
    "    print(\"No hay checkpoints.\")\n",
    "    last_epoch = 0  # Comienza desde la primera época\n",
    "\n",
    "final_model.fit(\n",
    "    X_train_aug,\n",
    "    epochs=100,\n",
    "    validation_data=val_ds,\n",
    "    initial_epoch=last_epoch,\n",
    "    callbacks=[ model_checkpoint_callback, early_stopping, reduce_lr, csv_logger,tensorboard_callback],\n",
    "    \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cargar el modelo\n",
    "final_model=keras.saving.load_model('modelo_fin_effi.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "simple_aug = keras.Sequential([\n",
    "    layers.Resizing(96, 96),  # Redimensionar imágenes\n",
    "    layers.RandomFlip(\"horizontal\"),  # Volteo horizontal aleatorio\n",
    "    layers.RandomRotation(factor=0.02),  # Rotación aleatoria\n",
    "    layers.RandomZoom(height_factor=0.2, width_factor=0.2),  # Zoom aleatorio\n",
    "    layers.RandomContrast(0.2),  # Aumentar o reducir el contraste\n",
    "    layers.RandomTranslation(height_factor=0.1, width_factor=0.1),  # Desplazamiento aleatorio\n",
    "    layers.RandomBrightness(factor=0.2),  # Modificación del brillo\n",
    "    layers.GaussianNoise(0.05),  # Ruido gaussiano para simular imperfecciones\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para aplicar aumentación varias veces\n",
    "def augment_multiple_times(image, label, n=3):  # Generar 'n' variantes de cada imagen\n",
    "    augmented_images = [simple_aug(image) for _ in range(n)]\n",
    "    labels = [label for _ in range(n)]\n",
    "    return tf.data.Dataset.from_tensor_slices((augmented_images, labels))\n",
    "\n",
    "# Aplicar aumentación a cada imagen varias veces\n",
    "augmented_dataset = test_ds.flat_map(lambda x, y: augment_multiple_times(x, y, n=3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrar_imagenes(dataset, class_names):\n",
    "    \"\"\"\n",
    "    Muestra las primeras 9 imágenes de un lote del dataset junto con sus etiquetas.\n",
    "\n",
    "    Args:\n",
    "        dataset (tf.data.Dataset): Dataset cargado con image_dataset_from_directory o transformado.\n",
    "        class_names (list): Lista de nombres de clases.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for images, labels in dataset.take(1):  # Tomar un lote del dataset\n",
    "        for i in range(9):  # Mostrar las primeras 9 imágenes del lote\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "            # Convertir etiquetas categóricas en nombres de clases\n",
    "            plt.title(class_names[labels[i].numpy().argmax()])\n",
    "            plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Obtener los nombres de las clases desde el dataset original\n",
    "class_names = train_ds.class_names\n",
    "\n",
    "# Mostrar imágenes del dataset aumentado\n",
    "mostrar_imagenes(augmented_dataset, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 437ms/step - auc: 0.9135 - categorical_accuracy: 0.7542 - loss: 0.5675 - mae: 0.2069 - poisson: 0.5225 - precision: 0.8149 - recall: 0.6917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sebas\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6688915491104126,\n",
       " 0.7579618096351624,\n",
       " 0.6611111164093018,\n",
       " 0.8857793211936951,\n",
       " 0.2237929254770279,\n",
       " 0.7111111283302307,\n",
       " 0.5562970638275146]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.evaluate(augmented_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000023A8C5B3B50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step\n"
     ]
    }
   ],
   "source": [
    "y_pred=final_model.predict(augmented_dataset)\n",
    "y_pred_label=np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 405ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 378ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Obtener las etiquetas verdaderas y las predicciones del modelo\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for images, labels in augmented_dataset:\n",
    "    preds = final_model.predict(images)\n",
    "    predicted_labels.extend(np.argmax(preds, axis=1))  # Obtener la clase predicha\n",
    "    true_labels.extend(np.argmax(labels, axis=1))  # Obtener la clase verdadera\n",
    "\n",
    "true_labels = np.array(true_labels)\n",
    "predicted_labels = np.array(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Crear la matriz de confusión\n",
    "cm = tf.math.confusion_matrix(labels=true_labels, predictions=predicted_labels)\n",
    "\n",
    "# Convertir a numpy para facilitar la visualización\n",
    "cm = cm.numpy()\n",
    "\n",
    "# Visualizar la matriz de confusión\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(3), yticklabels=range(3))\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix Efficient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.save(\"modelo_fin_effi.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
